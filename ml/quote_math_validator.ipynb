{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e545771a",
   "metadata": {},
   "source": [
    "# Quote Math Validator\n",
    "\n",
    "This notebook validates the pricing math used by the form-builder quote engine.\n",
    "\n",
    "The PHP `QuoteEngine` and JavaScript `quote.js` share the same deterministic formula:\n",
    "\n",
    "```\n",
    "subtotal  = (base_rate * complexity_multiplier) + addon_total\n",
    "range_low  = round(subtotal * 0.9)\n",
    "range_high = round(subtotal * 1.2)\n",
    "```\n",
    "\n",
    "**Goals**\n",
    "1. Load `quote_math_validation.csv` \u2014 2 500+ correct rows + ~360 intentionally broken rows.\n",
    "2. Re-implement the formula in Python and assert it matches the CSV ground truth.\n",
    "3. Train a lightweight `DecisionTreeClassifier` to detect rows where the stored numbers are wrong.\n",
    "4. Expose a single `predict_quote(...)` function that returns `True` when the math checks out.\n",
    "\n",
    "All packages used (`pandas`, `numpy`, `scikit-learn`) are available on the Kaggle free tier with no GPU required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061dc876",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"pandas\", pd.__version__, \"| numpy\", np.__version__)\n",
    "\n",
    "import sklearn; print(\"scikit-learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dfbc42",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "`quote_math_validation.csv` was generated by `gen_data.py` which:\n",
    "- Enumerates every `service \u00d7 complexity` pair\n",
    "- Pairs each with no addons, each single addon, every 2-addon combination, every 3-addon combination, and a sample of 4- and 5-addon combinations\n",
    "- Adds ~15% intentionally miscalculated rows (wrong multiplier, wrong range_low, or wrong range_high) labelled `math_correct = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f560e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess, sys\n",
    "\n",
    "KAGGLE_BASE = Path(\"/kaggle/input/datasets/crissymoon/quote-math-validation\")\n",
    "KAGGLE_CSV  = KAGGLE_BASE / \"quote_math_validation.csv\"\n",
    "KAGGLE_GEN  = KAGGLE_BASE / \"gen_data.py\"\n",
    "LOCAL_BASE  = Path(\"/kaggle/working\")\n",
    "LOCAL_CSV   = LOCAL_BASE / \"quote_math_validation.csv\"\n",
    "\n",
    "# Priority: Kaggle input dataset -> already-generated working copy -> regenerate now\n",
    "if KAGGLE_CSV.exists():\n",
    "    CSV_PATH = KAGGLE_CSV\n",
    "elif LOCAL_CSV.exists():\n",
    "    CSV_PATH = LOCAL_CSV\n",
    "else:\n",
    "    gen_script = KAGGLE_GEN if KAGGLE_GEN.exists() else None\n",
    "    if gen_script:\n",
    "        print(\"Regenerating CSV from gen_data.py ...\")\n",
    "        subprocess.run([sys.executable, str(gen_script)], check=True)\n",
    "        CSV_PATH = LOCAL_CSV\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"CSV not found.\\nExpected: {KAGGLE_CSV}\\n\"\n",
    "            \"Attach the dataset 'crissymoon/quote-math-validation' to this notebook.\"\n",
    "        )\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"CSV loaded from:\", CSV_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nClass balance:\")\n",
    "print(df[\"math_correct\"].value_counts())\n",
    "print(\"\\nError type distribution:\")\n",
    "print(df[\"error_type\"].value_counts())\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cdbe8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e914c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.isnull().sum().sum() == 0, \"Unexpected nulls \u2014 re-run gen_data.py\"\n",
    "\n",
    "le_service    = LabelEncoder().fit(df[\"service_type\"])\n",
    "le_complexity = LabelEncoder().fit(df[\"complexity\"])\n",
    "\n",
    "df[\"service_enc\"]    = le_service.transform(df[\"service_type\"])\n",
    "df[\"complexity_enc\"] = le_complexity.transform(df[\"complexity\"])\n",
    "\n",
    "print(\"Service classes   :\", list(le_service.classes_))\n",
    "print(\"Complexity classes:\", list(le_complexity.classes_))\n",
    "\n",
    "ADDON_COLS = [c for c in df.columns if c.startswith(\"addon_\")]\n",
    "NUM_COLS   = [\"base_rate\", \"complexity_multiplier\", \"addon_total\",\n",
    "              \"subtotal\", \"range_low\", \"range_high\"]\n",
    "CAT_COLS   = [\"service_enc\", \"complexity_enc\"]\n",
    "FEATURES   = CAT_COLS + NUM_COLS + ADDON_COLS\n",
    "TARGET     = \"math_correct\"\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}  Test: {len(X_test)}\")\n",
    "print(f\"Train positive rate: {y_train.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95be96",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "The numeric columns already encode everything the formula uses, but we add three derived features that make the decision boundary trivially learnable for the tree:\n",
    "\n",
    "| Derived feature | Formula |\n",
    "|---|---|\n",
    "| `expected_subtotal` | `base_rate * complexity_multiplier + addon_total` |\n",
    "| `low_delta` | `range_low - round(expected_subtotal * 0.9)` |\n",
    "| `high_delta` | `range_high - round(expected_subtotal * 1.2)` |\n",
    "\n",
    "A correct row has `low_delta == 0` and `high_delta == 0`. The model learns this quickly, but the exercise is still useful for catching float-rounding edge cases or JS/PHP divergence you might introduce later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_derived(frame):\n",
    "    \"\"\"Add expected_subtotal, low_delta, high_delta to a copy of the frame.\"\"\"\n",
    "    f = frame.copy()\n",
    "    f[\"expected_subtotal\"] = (\n",
    "        f[\"base_rate\"] * f[\"complexity_multiplier\"] + f[\"addon_total\"]\n",
    "    )\n",
    "    f[\"low_delta\"]  = f[\"range_low\"]  - (f[\"expected_subtotal\"] * 0.9).round()\n",
    "    f[\"high_delta\"] = f[\"range_high\"] - (f[\"expected_subtotal\"] * 1.2).round()\n",
    "    return f\n",
    "\n",
    "X_train_fe = add_derived(X_train)\n",
    "X_test_fe  = add_derived(X_test)\n",
    "\n",
    "FEATURES_FE = FEATURES + [\"expected_subtotal\", \"low_delta\", \"high_delta\"]\n",
    "\n",
    "print(\"Features:\", FEATURES_FE)\n",
    "print(\"Train shape:\", X_train_fe[FEATURES_FE].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d0862",
   "metadata": {},
   "source": [
    "## 5. Build the Lightweight Model\n",
    "\n",
    "A shallow `DecisionTreeClassifier` (`max_depth=5`) is used because:\n",
    "- The pricing formula is entirely deterministic \u2014 a tree of depth 2\u20133 should already reach near-100% accuracy.\n",
    "- It is fast, interpretable, and produces no float precision issues.\n",
    "- It can be serialised to a tiny JSON/dict for use inside the PHP or JS layer if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "\n",
    "print(\"Model:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ab6a5",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_fe[FEATURES_FE], y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, model.predict(X_train_fe[FEATURES_FE]))\n",
    "print(f\"Training accuracy : {train_acc:.4f}\")\n",
    "print(f\"Tree depth used   : {model.get_depth()}\")\n",
    "print(f\"Leaf nodes        : {model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d9495",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_fe[FEATURES_FE])\n",
    "\n",
    "print(\"Test set results\")\n",
    "print(\"----------------\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"wrong (0)\", \"correct (1)\"]))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"actual: wrong\", \"actual: correct\"],\n",
    "    columns=[\"pred: wrong\", \"pred: correct\"],\n",
    ")\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "print(f\"\\nAccuracy : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall   : {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1       : {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance = pd.Series(model.feature_importances_, index=FEATURES_FE)\n",
    "importance_sorted = importance[importance > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"Feature importances (non-zero only):\")\n",
    "print(importance_sorted.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3fbae6",
   "metadata": {},
   "source": [
    "## 8. Run Inference on New Form Inputs\n",
    "\n",
    "`predict_quote(...)` mirrors the `QuoteEngine::calculate()` PHP signature.  \n",
    "Pass the raw form values and it returns `True` if the numbers check out, `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_RATES = {\n",
    "    \"web_design\": 1500, \"web_development\": 3500, \"ecommerce\": 4500,\n",
    "    \"software\": 7500, \"ai_web_app\": 9500, \"ai_native_app\": 14000,\n",
    "}\n",
    "COMPLEXITY_MULTIPLIERS = {\n",
    "    \"simple\": 1.0, \"moderate\": 1.4, \"complex\": 2.0, \"custom\": 2.8,\n",
    "}\n",
    "ADDON_RATES = {\n",
    "    \"seo_basic\": 500, \"seo_advanced\": 1200, \"copywriting\": 800,\n",
    "    \"branding\": 1800, \"maintenance\": 1200, \"hosting_setup\": 350,\n",
    "    \"api_integration\": 1500, \"automation\": 2200,\n",
    "}\n",
    "ADDONS_LIST = list(ADDON_RATES.keys())\n",
    "\n",
    "\n",
    "def predict_quote(\n",
    "    service_type: str,\n",
    "    complexity: str,\n",
    "    addons: list,\n",
    "    claimed_subtotal: int,\n",
    "    claimed_range_low: int,\n",
    "    claimed_range_high: int,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the claimed figures match the QuoteEngine formula exactly.\n",
    "\n",
    "    Parameters mirror the PHP QuoteEngine::calculate() method.\n",
    "    The call also runs the trained model as a secondary check; both must agree.\n",
    "    \"\"\"\n",
    "    base       = BASE_RATES[service_type]\n",
    "    multiplier = COMPLEXITY_MULTIPLIERS[complexity]\n",
    "    addon_total = sum(ADDON_RATES[a] for a in addons if a in ADDON_RATES)\n",
    "    expected_subtotal = round(base * multiplier + addon_total)\n",
    "    expected_low      = round(expected_subtotal * 0.9)\n",
    "    expected_high     = round(expected_subtotal * 1.2)\n",
    "\n",
    "    # Deterministic rule check (no ML needed for production)\n",
    "    rule_ok = (\n",
    "        claimed_subtotal   == expected_subtotal and\n",
    "        claimed_range_low  == expected_low      and\n",
    "        claimed_range_high == expected_high\n",
    "    )\n",
    "\n",
    "    # ML model check (secondary)\n",
    "    addon_flags = {f\"addon_{a}\": (1 if a in addons else 0) for a in ADDONS_LIST}\n",
    "    row = {\n",
    "        \"service_enc\"            : le_service.transform([service_type])[0],\n",
    "        \"complexity_enc\"         : le_complexity.transform([complexity])[0],\n",
    "        **{f\"addon_{a}\": addon_flags[f\"addon_{a}\"] for a in ADDONS_LIST},\n",
    "        \"base_rate\"              : base,\n",
    "        \"complexity_multiplier\"  : multiplier,\n",
    "        \"addon_total\"            : addon_total,\n",
    "        \"subtotal\"               : claimed_subtotal,\n",
    "        \"range_low\"              : claimed_range_low,\n",
    "        \"range_high\"             : claimed_range_high,\n",
    "        \"expected_subtotal\"      : base * multiplier + addon_total,\n",
    "        \"low_delta\"              : claimed_range_low  - round((base * multiplier + addon_total) * 0.9),\n",
    "        \"high_delta\"             : claimed_range_high - round((base * multiplier + addon_total) * 1.2),\n",
    "    }\n",
    "    row_df   = pd.DataFrame([row])[FEATURES_FE]\n",
    "    model_ok = bool(model.predict(row_df)[0])\n",
    "\n",
    "    return rule_ok and model_ok\n",
    "\n",
    "\n",
    "# --- Test cases ---\n",
    "cases = [\n",
    "    # Correct: AI Web App, complex, branding + api_integration\n",
    "    dict(service_type=\"ai_web_app\", complexity=\"complex\",\n",
    "         addons=[\"branding\", \"api_integration\"],\n",
    "         claimed_subtotal=22300, claimed_range_low=20070, claimed_range_high=26760),\n",
    "\n",
    "    # Correct: web_design, simple, no addons\n",
    "    dict(service_type=\"web_design\", complexity=\"simple\",\n",
    "         addons=[], claimed_subtotal=1500, claimed_range_low=1350, claimed_range_high=1800),\n",
    "\n",
    "    # Wrong: web_development, moderate, wrong range_high\n",
    "    dict(service_type=\"web_development\", complexity=\"moderate\",\n",
    "         addons=[\"seo_basic\"],\n",
    "         claimed_subtotal=5400, claimed_range_low=4860, claimed_range_high=9999),\n",
    "]\n",
    "\n",
    "print(f\"{'service':<20} {'complexity':<10} {'addons':<25} {'result'}\")\n",
    "print(\"-\" * 75)\n",
    "for c in cases:\n",
    "    ok = predict_quote(**c)\n",
    "    print(f\"{c['service_type']:<20} {c['complexity']:<10} {str(c['addons']):<25} {'CORRECT' if ok else 'WRONG'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}